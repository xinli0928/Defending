{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as func\n",
    "from torchvision import models\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "from GCE import *\n",
    "from advertorch.attacks import  GradientSignAttack,LinfPGDAttack\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'data/'\n",
    "traindir = datadir + 'train/'\n",
    "testdir = datadir + 'test/'\n",
    "valdir = datadir + 'val/'\n",
    "unlabeldir = datadir + 'unlabel/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor()]),\n",
    "    'unlabel':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor()]),\n",
    "    'val':\n",
    "        transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor()]),\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data = {'train':datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n",
    "        'unlabel':datasets.ImageFolder(root=unlabeldir, transform=image_transforms['unlabel']),\n",
    "        'val':datasets.ImageFolder(root=valdir, transform=image_transforms['val'])}\n",
    "        \n",
    "# Dataloader iterators\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True, num_workers =8, pin_memory = True),\n",
    "    'unlabel': DataLoader(data['unlabel'], batch_size=batch_size, shuffle=True, num_workers =8, pin_memory = True),\n",
    "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=False,num_workers =8,pin_memory = True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 4)\n",
    "input_size = 224\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = GuidedComplementEntropy(0.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    size = 0 \n",
    "    for tedata, tetarget in dataloaders['val']:\n",
    "        size += tedata.shape[0]\n",
    "        tedata, tetarget = tedata.to(device), tetarget.to(device)\n",
    "\n",
    "        output = model(tedata)\n",
    "        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(tetarget.view_as(pred)).sum().item()\n",
    "\n",
    "    print(\"{:s} acc: {:.2f}\".format('clean', 100. * correct / size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    #copy_weights\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "    for inputs, labels in dataloaders['train']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # eps_now= np.random.uniform(0.001,0.003)\n",
    "        # FGSM = GradientSignAttack(model, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),eps=eps_now)\n",
    "        # advdata = FGSM.perturb(inputs, labels).to(device)\n",
    "        # inputs= torch.cat((inputs, advdata),0)\n",
    "        # labels= torch.cat((labels, labels))\n",
    "\n",
    "        #generate attacks\n",
    "        iterator = iter(dataloaders['unlabel'])\n",
    "        inputs_u,labels_u = iterator.next()\n",
    "        inputs_u = inputs_u.to(device)\n",
    "        labels_u = labels_u.to(device)\n",
    "\n",
    "        eps_now= np.random.uniform(0.001,0.003)\n",
    "        FGSM = GradientSignAttack(model, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),eps=eps_now)\n",
    "        inputs_u_adv = FGSM.perturb(inputs_u).to(device)\n",
    "        outputs_adv = model(inputs_u_adv)\n",
    "        _,labels_pseudo = torch.max(model(inputs_u),1)\n",
    "\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "#        softmax\n",
    "        loss1 = criterion(outputs, labels)\n",
    "        loss2 = criterion(outputs_adv, labels_pseudo)\n",
    "        loss = loss1 +3*loss2\n",
    "        #loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloaders['train'].dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloaders['train'].dataset)\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('train', epoch_loss, epoch_acc))\n",
    "    val(model)\n",
    "    print()\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in range(11):\n",
    "    correct = 0\n",
    "    size = 0\n",
    "    n = 0\n",
    "    for tedata, tetarget in dataloaders['test']:\n",
    "        size += tedata.shape[0]\n",
    "        tedata, tetarget = tedata.to(device), tetarget.to(device)\n",
    "\n",
    "        \n",
    "        eps_now = i*0.0002\n",
    "        pgd = GradientSignAttack(model, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),eps=eps_now)\n",
    "        tedata = pgd.perturb(tedata, tetarget).to(device)\n",
    "        output = model(tedata)\n",
    "        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(tetarget.view_as(pred)).sum().item()\n",
    "\n",
    "    print(\"{:f} acc: {:.2f}\".format(i, 100. * correct / size)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}